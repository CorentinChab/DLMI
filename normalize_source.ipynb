{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebc42a9b-4c32-4385-97bf-81de4e1ed35b",
   "metadata": {},
   "source": [
    "# Center Specific Normalization and Ensemble Prediction\n",
    "\n",
    "In this notebook, we propose to investigate the advantage to use center specific normalization instead of using individual normalisation. To be able to compare the advantage of this method, we use the same baseline model and only change the train test and valid dataset. We also investigate aggregating several predictions on augmented data to reduce the variance of the predicitions.\n",
    "\n",
    "This method gives promising results, leading to an accuracy of 93.5% on the test set (where the baseline model achieved 90.6%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63498aaa-a8fd-4c8d-92f2-1054c567f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchmetrics\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb42acc9-c3fd-4e42-83c3-9b231f119ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_PATH = 'train.h5'\n",
    "VAL_IMAGES_PATH = 'val.h5'\n",
    "TEST_IMAGES_PATH = 'test.h5'\n",
    "SEED = 0\n",
    "GLOBAL_MEAN = np.array([0.485, 0.456, 0.406])\n",
    "GLOBAL_STD = np.array([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bbe3356-6c15-43b0-80b4-fae3ea7c0951",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.random.manual_seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e76c4d-7d3e-4db2-b43a-03c4c310b0fc",
   "metadata": {},
   "source": [
    "## 1. Building a center normalized dataset\n",
    "The datasets will leverage statistics computed in each center to be able to normalized the data according to the center it comes from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24d42e7-1e30-4c84-8b75-8eaa86e31580",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387e1a75",
   "metadata": {},
   "source": [
    "Normalizing according to the center can be better than normalizing images individually because doing so would erase outlyer information. Here, we can know how the image behaves relatively to other obtained in the same condition, which could preserve outlier info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18000d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_center_stats(dataset_paths):\n",
    "        \"\"\"Compute mean and std for each center in the dataset\"\"\"\n",
    "        center_stats = {}\n",
    "        for dataset_path in dataset_paths:\n",
    "            with h5py.File(dataset_path, 'r') as hdf:\n",
    "                # First we group images by center\n",
    "                center_images = {}\n",
    "                for img_id in hdf.keys():\n",
    "                    try:\n",
    "                        center = int(np.array(hdf.get(img_id).get('metadata'))[0])\n",
    "                        if center not in center_images:\n",
    "                            center_images[center] = []\n",
    "                        center_images[center].append(np.array(hdf.get(img_id).get('img')))\n",
    "                    except:\n",
    "                        continue\n",
    "\n",
    "        # Compute mean and std for each center\n",
    "        for center, images in center_images.items():\n",
    "            all_images = np.vstack([img.reshape(1, *img.shape) for img in images])\n",
    "            mean = np.mean(all_images, axis=(0, 2, 3))\n",
    "            std = np.std(all_images, axis=(0, 2, 3))\n",
    "            center_stats[center] = {'mean': mean, 'std': std}\n",
    "                    \n",
    "        return center_stats\n",
    "\n",
    "class CenterNormalizedDataset(Dataset):\n",
    "    def __init__(self, dataset_path, preprocessing, mode, center_stats=None):\n",
    "        super(CenterNormalizedDataset, self).__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.preprocessing = preprocessing\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Pre-compute center statistics if not provided\n",
    "        self.center_stats = center_stats\n",
    "        if self.center_stats is None:\n",
    "            self.center_stats = compute_center_stats([dataset_path])\n",
    "        \n",
    "        with h5py.File(self.dataset_path, 'r') as hdf:        \n",
    "            self.image_ids = list(hdf.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        with h5py.File(self.dataset_path, 'r') as hdf:\n",
    "            img = np.array(hdf.get(img_id).get('img'))\n",
    "            center = int(np.array(hdf.get(img_id).get('metadata'))[0])\n",
    "            label = np.array(hdf.get(img_id).get('label')) if self.mode == 'train' else None\n",
    "        \n",
    "        # Apply normalization to match global stats\n",
    "        if center in self.center_stats:\n",
    "            # First, standardize using center-specific stats\n",
    "            center_mean = self.center_stats[center]['mean']\n",
    "            center_std = self.center_stats[center]['std']\n",
    "            \n",
    "            # Standardize to zero mean and unit variance\n",
    "            img_standardized = (img - center_mean.reshape(-1, 1, 1)) / (center_std.reshape(-1, 1, 1) + 1e-8)\n",
    "            \n",
    "            # Rescale to global statistics\n",
    "            img = img_standardized * GLOBAL_STD.reshape(-1, 1, 1) + GLOBAL_MEAN.reshape(-1, 1, 1)\n",
    "        \n",
    "        img_tensor = torch.tensor(img)\n",
    "        if self.preprocessing:\n",
    "            img_tensor = self.preprocessing(img_tensor)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            return img_tensor.float(), torch.tensor(label).float()\n",
    "        else:\n",
    "            return img_tensor.float(), None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52d9efc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "center stats computed\n"
     ]
    }
   ],
   "source": [
    "preprocessing = transforms.Resize((98, 98))\n",
    "\n",
    "# Compute stats from all data\n",
    "center_stats = compute_center_stats([TRAIN_IMAGES_PATH, VAL_IMAGES_PATH, TEST_IMAGES_PATH])\n",
    "print(\"center stats computed\")\n",
    "\n",
    "# Pass the computed center statistics to the datasets\n",
    "train_dataset = CenterNormalizedDataset(TRAIN_IMAGES_PATH, preprocessing, 'train', center_stats)\n",
    "val_dataset = CenterNormalizedDataset(VAL_IMAGES_PATH, preprocessing, 'train', center_stats)\n",
    "test_dataset = CenterNormalizedDataset(TEST_IMAGES_PATH, preprocessing, 'eval', center_stats)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f7d863",
   "metadata": {},
   "source": [
    "Since we will use the same model as the baseline proposed (Dinov2 + linear classifier), we will use their PrecomputedDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "583e0071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute(dataloader, model, device):\n",
    "    xs, ys = [], []\n",
    "    for x, y in tqdm(dataloader, leave=False):\n",
    "        with torch.no_grad():\n",
    "            # Make sure x has the correct shape (B, C, H, W)\n",
    "            if len(x.shape) == 3:  # If missing batch dimension\n",
    "                x = x.unsqueeze(0)\n",
    "            # Extract features\n",
    "            features = model(x.to(device)).detach().cpu()\n",
    "            xs.append(features.numpy())\n",
    "        ys.append(y.numpy() if y is not None else np.zeros(x.size(0)))\n",
    "    xs = np.vstack(xs)\n",
    "    ys = np.hstack(ys)\n",
    "    return torch.tensor(xs), torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba7ed6b",
   "metadata": {},
   "source": [
    "This dataset is used to store precomputed features on which we can train a classifier only model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82d0da0-1632-49b7-8b56-6b0e7212f318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrecomputedDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        super(PrecomputedDataset, self).__init__()\n",
    "        self.features = features\n",
    "        self.labels = labels.unsqueeze(-1)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx].float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec43b5-32a8-4a02-8794-28f33291133e",
   "metadata": {},
   "source": [
    "## 2. Precomputing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2936e5f2-8702-4b52-987a-02765194f261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on cuda.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Working on {device}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1026000",
   "metadata": {},
   "source": [
    "As in the baseline model, we use dinov2 to precompute the features. Our main contribution in this notebook is to propose and test a center specific normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c622bc89-5505-4cb7-b38b-ed86b081b8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /raid/home/automatants/tabbara_pau/.cache/torch/hub/facebookresearch_dinov2_main\n",
      "/raid/home/automatants/tabbara_pau/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/raid/home/automatants/tabbara_pau/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/raid/home/automatants/tabbara_pau/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
    "feature_extractor.eval()\n",
    "linear_probing = torch.nn.Sequential(torch.nn.Linear(feature_extractor.num_features, 1),\n",
    "                                     torch.nn.Sigmoid()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "901a3bba-b7cc-4a7c-80b3-a4dfe5170429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f606145a6d4a422ebc2ecdba5513fbe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/automatants/tabbara_pau/dlip/dlip_venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994e192f315643a59a106baf7d32f0a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = PrecomputedDataset(*precompute(train_dataloader, feature_extractor, device))\n",
    "val_dataset = PrecomputedDataset(*precompute(val_dataloader, feature_extractor, device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "79a4df69-cd6a-44cd-bcba-8c0db7422a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(train_dataset, 'train_dataset_center_normalized.pth')\n",
    "torch.save(val_dataset, 'val_dataset_center_normalized.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66323123-d450-4a91-b712-0b1cb364923d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9f30c-12da-4514-8550-f86209e34efb",
   "metadata": {},
   "source": [
    "## 3.a) Training the model on center-normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ef85d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044c5028ada4475da3de763a235c6038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [1/100] | Loss 0.1842 | Metric 0.9295\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "515baf636c204f659983fa8bfac915f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [1/100] | Loss 0.3431 | Metric 0.8563\n",
      "New best loss inf -> 0.3431\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecdae53aefaf427b8a432913f86cc120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [2/100] | Loss 0.1555 | Metric 0.9415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bc7d9827ff4110aa80300577b02c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [2/100] | Loss 0.3224 | Metric 0.8647\n",
      "New best loss 0.3431 -> 0.3224\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c72b8db8936149dfbb2545a3904b7e25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [3/100] | Loss 0.1506 | Metric 0.9426\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8411b21d3007472aa8163802dcce497e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [3/100] | Loss 0.3816 | Metric 0.8499\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df692354dc14533b4657ba489a3572d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [4/100] | Loss 0.1475 | Metric 0.9446\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ac21c18b5b84117a789ecb004d7d0c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [4/100] | Loss 0.3955 | Metric 0.8449\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6147a2689a049fc97692bd1ffbc1eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [5/100] | Loss 0.1461 | Metric 0.9454\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff37923efb454a3c916fbbe7f99776ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [5/100] | Loss 0.3109 | Metric 0.8724\n",
      "New best loss 0.3224 -> 0.3109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3595adc93fb4877bf874e55ec69bc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [6/100] | Loss 0.1443 | Metric 0.9459\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70dfcc8a01642dda71e082e7e53b14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [6/100] | Loss 0.3332 | Metric 0.8663\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1d07255409b43ffa822d9492de9244e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [7/100] | Loss 0.1428 | Metric 0.9464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb20fa23f6646fd99f8fd028844ce9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [7/100] | Loss 0.3201 | Metric 0.8713\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8ff3c65c324d7f885b0fc402e1016a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [8/100] | Loss 0.1424 | Metric 0.9464\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d66d95a188f482d96024a29c1c09152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [8/100] | Loss 0.3451 | Metric 0.8636\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436641f2dbbf46d3b049ad17b907d261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [9/100] | Loss 0.1421 | Metric 0.9467\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b5c497fbcb408889ba5ff913acd816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [9/100] | Loss 0.3212 | Metric 0.8735\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff588ee9845480abc6c06128479edcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [10/100] | Loss 0.1410 | Metric 0.9472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06c536485e140dbac99cb41b86333b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [10/100] | Loss 0.3276 | Metric 0.8699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e94ed42d04348f29f084788565b972e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [11/100] | Loss 0.1407 | Metric 0.9472\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d0e6e1195f42d298d12b5c6188de75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [11/100] | Loss 0.3447 | Metric 0.8659\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43df993a31964ba7b2c347958159caf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [12/100] | Loss 0.1410 | Metric 0.9467\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb907d475fee457a9116edbaa42a8087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [12/100] | Loss 0.3177 | Metric 0.8756\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6461b70c90c41509a8bda500cabf7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [13/100] | Loss 0.1404 | Metric 0.9477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3cfe0edb51453cb68448bc1852d592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [13/100] | Loss 0.4696 | Metric 0.8245\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58273b985b2249c0b3e877291e4fb354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [14/100] | Loss 0.1405 | Metric 0.9477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78397b435ba44338423aee1cddbd774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [14/100] | Loss 0.3090 | Metric 0.8778\n",
      "New best loss 0.3109 -> 0.3090\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f185fb3785b946c1acc17f7678b7d237",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [15/100] | Loss 0.1396 | Metric 0.9481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27244ebd59347a7a8c18846195c94fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [15/100] | Loss 0.3310 | Metric 0.8739\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d512a533225649edab1746c8f6f657cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [16/100] | Loss 0.1405 | Metric 0.9476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7dddf563e1345d7a74d9056dc86a2dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [16/100] | Loss 0.3344 | Metric 0.8640\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746ca572c1904e9b9796b4fca8ca0e50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [17/100] | Loss 0.1399 | Metric 0.9480\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45e715bb33ef42f681b550abe597503e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [17/100] | Loss 0.3121 | Metric 0.8784\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e00f3f9c84184696801ecd8a15053b0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [18/100] | Loss 0.1394 | Metric 0.9478\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d1d97ddd766417299aa1e67827d9114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [18/100] | Loss 0.3463 | Metric 0.8666\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed0e9834cb54175a603d491bce52855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [19/100] | Loss 0.1391 | Metric 0.9480\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6db42819bc064d23a764d8f2a36108fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [19/100] | Loss 0.3402 | Metric 0.8699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e818e5edc74576a7754f017bb9cb42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [20/100] | Loss 0.1400 | Metric 0.9477\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083e239206db4a79b7e1dd8abaee467f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [20/100] | Loss 0.3279 | Metric 0.8689\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c72edf29b6f40b5902b08e8e54aa14e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [21/100] | Loss 0.1392 | Metric 0.9475\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656333ac5f1740d1873e5f6317526a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [21/100] | Loss 0.3153 | Metric 0.8757\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82519d681f854442b86abf0be02daeb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [22/100] | Loss 0.1392 | Metric 0.9479\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e03e2265c6f45bb9155aa0e78a65ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [22/100] | Loss 0.3863 | Metric 0.8536\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d0b0c7d06642db84296b57c54d7680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [23/100] | Loss 0.1390 | Metric 0.9481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbf3cbcd7cd4e5cb844a3a85c58e613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [23/100] | Loss 0.3341 | Metric 0.8737\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "977e651a388b4813b3c6150779379eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [24/100] | Loss 0.1393 | Metric 0.9481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7300363d3b44bab87334e001439f5f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1091 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [24/100] | Loss 0.3507 | Metric 0.8676\n",
      "Early stopping after 10 epochs without improvement\n"
     ]
    }
   ],
   "source": [
    "def training_pipeline(feature_extractor, device, train_dataloader, val_dataloader, num_epochs=100, patience=10):\n",
    "    \n",
    "    linear_probing = torch.nn.Sequential(\n",
    "        torch.nn.Linear(feature_extractor.num_features, 1),\n",
    "        torch.nn.Sigmoid()\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(linear_probing.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    metric = torchmetrics.Accuracy('binary')\n",
    "    \n",
    "    min_loss, best_epoch = float('inf'), 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        linear_probing.train()\n",
    "        train_metrics, train_losses = [], []\n",
    "        \n",
    "        for train_x, train_y in tqdm(train_dataloader, leave=False):\n",
    "            optimizer.zero_grad()\n",
    "            train_pred = linear_probing(train_x.to(device))\n",
    "            loss = criterion(train_pred, train_y.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.extend([loss.item()]*len(train_y))\n",
    "            train_metric = metric(train_pred.cpu(), train_y.int().cpu())\n",
    "            train_metrics.extend([train_metric.item()]*len(train_y))\n",
    "        \n",
    "        print(f'Epoch train [{epoch+1}/{num_epochs}] | Loss {np.mean(train_losses):.4f} | Metric {np.mean(train_metrics):.4f}')\n",
    "        \n",
    "        linear_probing.eval()\n",
    "        val_metrics, val_losses = [], []\n",
    "        \n",
    "        for val_x, val_y in tqdm(val_dataloader, leave=False):\n",
    "            with torch.no_grad():\n",
    "                val_pred = linear_probing(val_x.to(device))\n",
    "            \n",
    "            loss = criterion(val_pred, val_y.to(device))\n",
    "            val_losses.extend([loss.item()]*len(val_y))\n",
    "            val_metric = metric(val_pred.cpu(), val_y.int().cpu())\n",
    "            val_metrics.extend([val_metric.item()]*len(val_y))\n",
    "        \n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "        print(f'Epoch valid [{epoch+1}/{num_epochs}] | Loss {mean_val_loss:.4f} | Metric {np.mean(val_metrics):.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if mean_val_loss < min_loss:\n",
    "            print(f'New best loss {min_loss:.4f} -> {mean_val_loss:.4f}')\n",
    "            min_loss = mean_val_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(linear_probing.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch - best_epoch == patience:\n",
    "            print(f'Early stopping after {patience} epochs without improvement')\n",
    "            break\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    linear_probing.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    return linear_probing\n",
    "\n",
    "train_dataset = torch.load('train_dataset_center_normalized.pth')\n",
    "val_dataset = torch.load('val_dataset_center_normalized.pth')\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "best_model = training_pipeline(\n",
    "    feature_extractor=feature_extractor,\n",
    "    device=device,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    num_epochs=100,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7b0122",
   "metadata": {},
   "source": [
    "## 3.b) Training with simply Imagenet normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852531d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOv2Dataset(Dataset):\n",
    "    def __init__(self, dataset_path, mode='train'):\n",
    "        super(DINOv2Dataset, self).__init__()\n",
    "        self.dataset_path = dataset_path\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Standard ImageNet normalization used by DINOv2\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            )\n",
    "        ])\n",
    "        \n",
    "        with h5py.File(self.dataset_path, 'r') as hdf:        \n",
    "            self.image_ids = list(hdf.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        with h5py.File(self.dataset_path, 'r') as hdf:\n",
    "            img = torch.tensor(np.array(hdf.get(img_id).get('img'))).float()\n",
    "            label = float(np.array(hdf.get(img_id).get('label'))) if self.mode == 'train' else 0.0\n",
    "        \n",
    "        # Ensure image values are in [0, 1] range\n",
    "        if img.max() > 1.0:\n",
    "            img = img / 255.0\n",
    "            \n",
    "        # Apply ImageNet normalization for DINOv2\n",
    "        normalized_img = self.transform(img)\n",
    "        \n",
    "        return normalized_img, torch.tensor(label)\n",
    "    \n",
    "def prepare_dinov2_dataloaders(batch_size=32):\n",
    "    # Just ImageNet normalization\n",
    "    train_dataset = DINOv2Dataset(TRAIN_IMAGES_PATH, mode='train')\n",
    "    val_dataset = DINOv2Dataset(VAL_IMAGES_PATH, mode='train')\n",
    "    test_dataset = DINOv2Dataset(TEST_IMAGES_PATH, mode='eval')\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "    val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ed518cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_pipeline_dinov2(feature_extractor, device, train_dataloader, val_dataloader, num_epochs=100, patience=10):\n",
    "    \n",
    "    linear_probing = torch.nn.Sequential(\n",
    "        torch.nn.Linear(feature_extractor.num_features, 1),\n",
    "        torch.nn.Sigmoid()\n",
    "    ).to(device)\n",
    "    \n",
    "    # Place feature extractor in eval mode since we're not training it\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(linear_probing.parameters(), lr=0.001)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    metric = torchmetrics.Accuracy('binary')\n",
    "    \n",
    "    min_loss, best_epoch = float('inf'), 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        linear_probing.train()\n",
    "        train_metrics, train_losses = [], []\n",
    "        \n",
    "        for train_x, train_y in tqdm(train_dataloader, leave=False):\n",
    "            # First extract features using DinoV2\n",
    "            with torch.no_grad():\n",
    "                train_x = train_x.to(device)\n",
    "                features = feature_extractor(train_x)\n",
    "            \n",
    "            # Reshape labels and move to device\n",
    "            train_y = train_y.float().unsqueeze(1).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            train_pred = linear_probing(features)  # Use the extracted features\n",
    "            loss = criterion(train_pred, train_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            train_metric = metric(train_pred.detach().cpu(), train_y.int().cpu())\n",
    "            train_metrics.append(train_metric.item())\n",
    "        \n",
    "        print(f'Epoch train [{epoch+1}/{num_epochs}] | Loss {np.mean(train_losses):.4f} | Metric {np.mean(train_metrics):.4f}')\n",
    "        \n",
    "        linear_probing.eval()\n",
    "        val_metrics, val_losses = [], []\n",
    "        \n",
    "        for val_x, val_y in tqdm(val_dataloader, leave=False):\n",
    "            with torch.no_grad():\n",
    "                val_x = val_x.to(device)\n",
    "                features = feature_extractor(val_x)\n",
    "                \n",
    "                # Reshape labels and move to device\n",
    "                val_y = val_y.float().unsqueeze(1).to(device)\n",
    "                val_pred = linear_probing(features)\n",
    "            \n",
    "            loss = criterion(val_pred, val_y)\n",
    "            val_losses.append(loss.item())\n",
    "            val_metric = metric(val_pred.cpu(), val_y.int().cpu())\n",
    "            val_metrics.append(val_metric.item())\n",
    "        \n",
    "        mean_val_loss = np.mean(val_losses)\n",
    "        print(f'Epoch valid [{epoch+1}/{num_epochs}] | Loss {mean_val_loss:.4f} | Metric {np.mean(val_metrics):.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if mean_val_loss < min_loss:\n",
    "            print(f'New best loss {min_loss:.4f} -> {mean_val_loss:.4f}')\n",
    "            min_loss = mean_val_loss\n",
    "            best_epoch = epoch\n",
    "            torch.save(linear_probing.state_dict(), 'best_model.pth')\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch - best_epoch == patience:\n",
    "            print(f'Early stopping after {patience} epochs without improvement')\n",
    "            break\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    linear_probing.load_state_dict(torch.load('best_model.pth'))\n",
    "    \n",
    "    return linear_probing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306d21cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d1931b96b24cde91720a6472f91600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/raid/home/automatants/tabbara_pau/dlip/dlip_venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [1/100] | Loss 0.1873 | Metric 0.9298\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7720d8cda204691ba0d7851078e4b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [1/100] | Loss 0.3811 | Metric 0.8337\n",
      "New best loss inf -> 0.3811\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1fd0df12c24c63a6e5661129e2b6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [2/100] | Loss 0.1348 | Metric 0.9504\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43439cf31bd14c4f9481a0a2c93a1629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [2/100] | Loss 0.3166 | Metric 0.8643\n",
      "New best loss 0.3811 -> 0.3166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea5099e225524c5da73b999ba03b875d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [3/100] | Loss 0.1258 | Metric 0.9541\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "033f0c1e3b2842c68117fa64a70142d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [3/100] | Loss 0.3031 | Metric 0.8741\n",
      "New best loss 0.3166 -> 0.3031\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2ca820cfb0475790ce27d3a94649ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch train [4/100] | Loss 0.1208 | Metric 0.9555\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a738f3a44aa40b880ec8b108d37ef4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/273 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch valid [4/100] | Loss 0.2987 | Metric 0.8795\n",
      "New best loss 0.3031 -> 0.2987\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c8f2e50b3a47f485a23087acb90150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = prepare_dinov2_dataloaders(batch_size=128)\n",
    "\n",
    "# Train with center awareness\n",
    "best_model = training_pipeline_dinov2(\n",
    "    feature_extractor=feature_extractor,\n",
    "    device=device,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    num_epochs=100,\n",
    "    patience=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c7a540-f9ab-414d-8ea9-29efb9fa18ee",
   "metadata": {},
   "source": [
    "## 4. Making the final prediction with ensemble method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d9748f-363f-41c9-8128-a2fa8ce47494",
   "metadata": {},
   "source": [
    "To create a solutions file, you need to generate a CSV with 2 columns.\n",
    "- **ID**: containing the ID of the image\n",
    "- **Pred**: with the predicted class (**threshold the prediction to get either 0 or 1**)\n",
    "- **Probability**: The output probability of a sample being labeled as 1.\n",
    "\n",
    "This part was the block that leaded to the greatest results improvements. THe idea is to run several inferences on augmented versions of test images and aggregate them for the final prediction.\n",
    "\n",
    "We keep the assigned probability to later adapt the classification threshold to gain ~1% in prediction accuracy. This is done in `change_csv.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a4a172e-0773-489d-a9b3-fab4033efab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_probing = torch.nn.Sequential(\n",
    "    torch.nn.Linear(feature_extractor.num_features, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ").to(device)\n",
    "linear_probing.load_state_dict(torch.load('best_model.pth', weights_only=True))\n",
    "linear_probing.eval()\n",
    "linear_probing.to(device)\n",
    "prediction_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f7d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_test_augmentation(img_tensor, num_augmentations=8):\n",
    "    augmented_images = [img_tensor]\n",
    "    \n",
    "    # Define possible augmentations\n",
    "    augmentations = [\n",
    "        # Rotations\n",
    "        lambda x: TF.rotate(x, 90),\n",
    "        lambda x: TF.rotate(x, 180),\n",
    "        lambda x: TF.rotate(x, 270),\n",
    "        # Flips\n",
    "        lambda x: TF.hflip(x),\n",
    "        lambda x: TF.vflip(x),\n",
    "        # Rots + Flips\n",
    "        lambda x: TF.hflip(TF.rotate(x, 90)),\n",
    "        lambda x: TF.vflip(TF.rotate(x, 90)),\n",
    "        lambda x: TF.hflip(TF.rotate(x, 180)),\n",
    "        lambda x: TF.vflip(TF.rotate(x, 180)),\n",
    "        # Color jitter\n",
    "        lambda x: TF.adjust_brightness(x, 1.2),\n",
    "        lambda x: TF.adjust_brightness(x, 0.8),\n",
    "        lambda x: TF.adjust_contrast(x, 1.2),\n",
    "        lambda x: TF.adjust_contrast(x, 0.8),\n",
    "    ]\n",
    "    \n",
    "    selected_augmentations = augmentations[:min(num_augmentations, len(augmentations))]\n",
    "    \n",
    "    for aug_func in selected_augmentations:\n",
    "        augmented_images.append(aug_func(img_tensor))\n",
    "    \n",
    "    return torch.stack(augmented_images)\n",
    "\n",
    "def normalize_for_dinov2(img_tensor):\n",
    "    if img_tensor.max() > 1.0:\n",
    "        img_tensor = img_tensor / 255.0\n",
    "        \n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "    return normalize(img_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2583bb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_ensemble_prediction(model, feature_extractor, img_tensor, device, num_augmentations=5, aggregation_method='mean', threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make prediction on one sample by aggregating predictoins on test augmentation\n",
    "    \"\"\"\n",
    "    augmented_images = apply_test_augmentation(img_tensor, num_augmentations)\n",
    "    augmented_images = torch.stack([normalize_for_dinov2(img) for img in augmented_images])\n",
    "    \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for aug_img in augmented_images:\n",
    "            features = feature_extractor(aug_img.unsqueeze(0).to(device))\n",
    "            pred = model(features).detach().cpu().item()\n",
    "            predictions.append(pred)\n",
    "    \n",
    "    if aggregation_method == 'mean':\n",
    "        final_prob = sum(predictions) / len(predictions)\n",
    "    elif aggregation_method == 'median':\n",
    "        final_prob = sorted(predictions)[len(predictions)//2]\n",
    "    elif aggregation_method == 'vote':\n",
    "        votes = [1 if p > threshold else 0 for p in predictions]\n",
    "        final_prob = sum(votes) / len(votes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown aggregation method: {aggregation_method}\")\n",
    "    \n",
    "    return int(final_prob > threshold), final_prob, predictions\n",
    "\n",
    "\n",
    "def all_enemble_predictions(feature_extractor, linear_probing, test_path, device, num_augmentations=5, aggregation_method='mean'):\n",
    "    \"\"\"Generate predictions with ensemble method for all test images\"\"\"\n",
    "    linear_probing.load_state_dict(torch.load('best_model.pth', map_location=device))\n",
    "    linear_probing.eval()\n",
    "    feature_extractor.eval()\n",
    "    \n",
    "    resize = transforms.Resize((224, 224))\n",
    "    \n",
    "    solutions_data = {'ID': [], 'Pred': [], 'Probability': []}\n",
    "    \n",
    "    with h5py.File(test_path, 'r') as hdf:\n",
    "        test_ids = list(hdf.keys())\n",
    "    \n",
    "    with h5py.File(test_path, 'r') as hdf:\n",
    "        for test_id in tqdm(test_ids):\n",
    "            img = torch.tensor(np.array(hdf.get(test_id).get('img'))).float()\n",
    "            img = resize(img)\n",
    "            \n",
    "            binary_pred, probability, all_predictions = single_ensemble_prediction(\n",
    "                linear_probing, \n",
    "                feature_extractor, \n",
    "                img, \n",
    "                device, \n",
    "                num_augmentations=num_augmentations,\n",
    "                aggregation_method=aggregation_method\n",
    "            )\n",
    "            \n",
    "            solutions_data['ID'].append(int(test_id))\n",
    "            solutions_data['Pred'].append(binary_pred)\n",
    "            solutions_data['Probability'].append(probability)\n",
    "            \n",
    "    solutions_df = pd.DataFrame(solutions_data).set_index('ID')\n",
    "    solutions_df.to_csv(f'tta_{aggregation_method}_{num_augmentations}.csv')\n",
    "    \n",
    "    return solutions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227f1a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "feature_extractor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14').to(device)\n",
    "linear_probing = torch.nn.Sequential(\n",
    "    torch.nn.Linear(feature_extractor.num_features, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ").to(device)\n",
    "\n",
    "solutions_df, aug_details = all_enemble_predictions(\n",
    "    feature_extractor=feature_extractor,\n",
    "    linear_probing=linear_probing,\n",
    "    test_path=TEST_IMAGES_PATH,\n",
    "    device=device,\n",
    "    num_augmentations=8,  # Use 8 different augmentations\n",
    "    aggregation_method='mean'\n",
    ")\n",
    "\n",
    "print(f\"Predictions complete, class distributions: {solutions_df['Pred'].value_counts()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
